# -*- coding: utf-8 -*-
"""HW4_hl3099.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xJd1ejGrhZPOeySyBBpAsEr4uiZOOBvY
"""

!pip install tensorflowjs   #install the package: tensorflow.js

import urllib
import re
from urllib.request import urlopen
import random
import numpy as np

"""## Problem 1: Text Classification in Browser
"""

"""### Step 1: Build a training set and a test set

I chose three of my favorite books (`Pride and Prejudice, Anna Karenina and Jane Eyre`)
from [Project Gutenberg](https://www.gutenberg.org/). And then I extracted 1000 sentences
from each book to be my training set. Moreover, in order to test the performance of model,
I also randomly chose 100 sentences from each book to be my test set.
"""

# Download three books from Gutenberg
b1='https://www.gutenberg.org/files/1342/1342-0.txt'     # Pride and Prejudice
b2='https://www.gutenberg.org/files/1399/1399-0.txt'       # Anna Karenina
b3='http://www.gutenberg.org/cache/epub/1260/pg1260.txt' # Jane Eyre

def get_content(url):
  text=urlopen(url).read().decode('utf-8')
  chap="(Chapter 1|CHAPTER I)"
  s=re.sub(chap,"<start>",text)
  s=s.split('<start>')[1:]

  sn=''
  for i in s:
    sn += i
  return sn

def clean_text(content):
  prefixes = "(Mr|St|Mrs|Ms|Dr)[.]"
  s=re.sub(prefixes,"\\1<p>",content)
  s=s.replace('<p>','')
  s= s.replace('\r\n\r\n\r\n','')
  s= s.replace('\r\n\r\n','<stop>')
  s= s.split('<stop>')
  s= [i for i in s if i!='']
  s= [i.replace('\r\n',' ') for i in s]
  s= [i.replace('_','') for i in s]
  s= [i.replace('鈥�','') for i in s]
  s= [i.replace('鈥�','') for i in s]
  s= [i.replace('""','') for i in s]
  s= [i.replace('""','') for i in s]

  return s[:1000], random.sample(s[1000:],100)

sent_lst1_train,sent_lst1_test=clean_text(get_content(b1))
sent_lst2_train,sent_lst2_test=clean_text(get_content(b2))
sent_lst3_train,sent_lst3_test=clean_text(get_content(b3))

"""### Step 2: Preprocess data"""

# Training data
X_train=list(sent_lst1_train+sent_lst2_train+sent_lst3_train)
y_train=list([0]*1000+[1]*1000+[2]*1000)

# Test data
X_test=list(sent_lst1_test+sent_lst2_test+sent_lst3_test)
y_test=list([0]*100+[1]*100+[2]*100)

"""And then in order to somehow prevent overfitting,
**I shuffled my training dataset and test dataset**."""

# Randomize Data(optional)
random.seed(1)
ind_train=random.sample(range(len(X_train)),len(X_train))
X_train_shuffled=[X_train[i] for i in ind_train]
y_train_shuffled=[y_train[i] for i in ind_train]

ind_test=random.sample(range(len(X_test)),len(X_test))
X_test_shuffled=[X_test[i] for i in ind_test]
y_test_shuffled=[y_test[i] for i in ind_test]

# Tokenize the documents
max_len = 100
num_words = 20000   #10000 is big enough so that it could gaurantee each word with an unique integer.
from keras.preprocessing.text import Tokenizer
# Fit the tokenizer on the training data
# Token: give each word with an index.
t = Tokenizer(num_words=num_words)
t.fit_on_texts(X_train)

print(t.word_index)

"""After getting the word index, we could vectorize our training dataset."""

vec_train = t.texts_to_sequences(X_train_shuffled)
print(vec_train)

"""Since sentences in these books do not have same length,
we could use padding technique so as to have same length sentences."""

from keras.preprocessing.sequence import pad_sequences
padded_train = pad_sequences(vec_train, maxlen=max_len, padding='post')

# Tokenize and pad test sequences
vec_test = t.texts_to_sequences(X_test_shuffled)
padded_test = pad_sequences(vec_test, maxlen=max_len, padding='post')

"""We will save the word index in metadata. Later,
we'll use it to convert words typed in the browser to numbers for prediction."""


"""### Step 3: Train a simple model——baseline"""

# A simple model
embedding_size = 8
n_classes = 3
epochs = 10

import keras
model = keras.Sequential()
model.add(keras.layers.Embedding(num_words, embedding_size, input_shape=(max_len,)))
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(3, activation='softmax'))
model.compile('adam', 'sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

model.fit(padded_train, y_train_shuffled, epochs=epochs,validation_split=0.2)

model.evaluate(padded_test,y_test_shuffled)

"""Based on the result above, it's clear that this simple model overfits our
training dataset severely. Next, we would try to build an advanced model——LSTM.

### Step 4: Train a LSTM model——advanced
"""

# An advanced model
embedding_size = 8
n_classes = 3
epochs = 3

import keras
adv_model = keras.Sequential()
adv_model.add(keras.layers.Embedding(num_words, embedding_size, input_shape=(max_len,)))
adv_model.add(keras.layers.Dropout(0.6))
adv_model.add(keras.layers.LSTM(64,return_sequences=True))
adv_model.add(keras.layers.Flatten())
adv_model.add(keras.layers.Dropout(0.6))
adv_model.add(keras.layers.Dense(3, activation='softmax'))
opt=keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
adv_model.compile(opt, 'sparse_categorical_crossentropy', metrics=['accuracy'])
adv_model.summary()

adv_model.fit(padded_train, y_train_shuffled, epochs=epochs,validation_split=0.2)

adv_model.evaluate(padded_test,y_test_shuffled)

"""Based on the result, we would see that our advanced model does not overfit too
badly as the simple model. And compared with the simple model,
**it increases the test accuracy by 6% and reduces the test loss by 10%**.
